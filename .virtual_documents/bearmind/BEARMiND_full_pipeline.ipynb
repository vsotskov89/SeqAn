


import shutil
import os

local_startup_dir = get_ipython().profile_dir.startup_dir
filedir = os.getcwd()
shutil.copy(os.path.join(filedir, 'startup.py'), os.path.join(local_startup_dir, 'startup.py'))


%run startup.py





import tempfile
tempfile.tempdir = '/export/home1/tmp'

config_data = {
    #'ROOT': '/home/vladimir/RawCalciumData/',
    'ROOT': '/export/home1/RawCalciumData/',
    'DATA_PATHWAY': 'bonsai'
}

update_config(config_data)


CONFIG


create_mouse_configs(root=CONFIG['ROOT'])
create_session_configs(root=CONFIG['ROOT'])





#Manual file selection:
fnames = list(askopenfilenames(title = 'Select files for inspection', initialdir = CONFIG['ROOT'], filetypes = [('AVI files', '.avi')]))
#OR, alternatively, you can use Automatic file selection
#fnames = glob(CONFIG['ROOT'] + folder_structure + '*.avi')

data = LoadSelectedVideos(fnames)
w = DrawCropper(data, fname=fnames[0])





#Batch crop
cpath_template = os.path.normpath(os.path.join(CONFIG['ROOT'], folder_structure, '*cropping.pickle'))
pick_names = glob(cpath_template)

print([get_session_name_from_path(fname) for fname in pick_names])
# TODO: read from mouse or sconfig only!

for name in pick_names:
    DoCropAndRewrite(name)
    extract_and_copy_ts(name)





#Automatic file selection
#fnames = glob(os.path.join(CONFIG['ROOT'], '*_CR.tif'))
#OR, alternatively, you can use manual file selection:
fnames = askopenfilenames(title = 'Select files for motion correction', initialdir = CONFIG['ROOT'], filetypes = [('TIFF files', '.tif')])

mc_dict = {
    'pw_rigid': False,         # flag for performing piecewise-rigid motion correction (otherwise just rigid)
    'max_shifts': (35, 35),    # maximum allowed rigid shift
    'gSig_filt': (8, 8),       # size of high pass spatial filtering, used in 1p data
    'strides': (48, 48),       # start a new patch for pw-rigid motion correction every x pixels
    'overlaps': (24, 24),      # overlap between pathes (size of patch strides+overlaps)
    'max_deviation_rigid': 15,  # maximum deviation allowed for patch with respect to rigid shifts
    'border_nan': 'copy',      # replicate values along the boundaries
    'use_cuda': True,          # Set to True in order to use GPU
    'memory_fact': CONFIG['RAM']/16.0,          # How much memory to allocate. 1 works for 16Gb, so 0.8 showd be optimized for 12Gb.
    'niter_rig': 1,
    'splits_rig': 20,          # for parallelization split the movies in  num_splits chuncks across time
                               # if none all the splits are processed and the movie is saved
    'num_splits_to_process_rig': None} # intervals at which patches are laid out for motion correction  

for name in tqdm.tqdm(fnames):
    DoMotionCorrection(name, mc_dict)
    CleanMemmaps(name)


ms_name = 'H02'
session_name = 'NOF_H02_0D'
mc_to_config = {'mc_params': mc_dict}

ms_config_path = get_mouse_config_path(ms_name)
session_config_path = get_session_config_path(session_name)

#update_config(mc_to_config, cpath=ms_config_path)
update_config(mc_to_config, cpath=session_config_path)








fname = askopenfilename(title = 'Select estimates file for examination',
                        initialdir = CONFIG['ROOT'],
                        filetypes = [('estimates files', '*estimates.pickle')])

print('estimates:', fname)
estimates = LoadEstimates(fname, default_fps=20)

# вот здесь надо руками вписать нужный тифф-файл, автоматизировать не нужно, т.к. структура папок везде разная
tifpath = "C:\\Users\\admin\\Projects\\H_mice\\HM_NOF_2D\\NOF_H04_4D_CR_MC.tif"

gsig=6
avim = build_average_image(tifpath, gsig)
estimates.imax = avim
out_name = fname.partition('_estimates')[0] + '_manual_imax_estimates.pickle'
print('edited estimates:', out_name)
with open(out_name, "wb") as f:
    pickle.dump(estimates, f)


plt.matshow(avim)


#fnames = glob(os.path.join(CONFIG['ROOT'], '*_MC.tif'))
#OR, alternatively, you can use manual file selection:
fnames = askopenfilenames(title = 'Select files for gsig testing', initialdir = CONFIG['ROOT'], filetypes = [('TIFF files', '.tif')])

#Test_gSig_Range(fnames[0])
print(fnames[0])
Test_gSig_Range(fnames[0],default_gsig = 3, maxframes = 2000)  # maxframes is the amount of frames taken into account, by default the whole file is to be taken, which may be too slow for large files


%matplotlib ipympl
#fnames = glob(os.path.join(CONFIG['ROOT'], '*_MC.tif'))
fnames = askopenfilenames(title = 'Select files for corr image testing',
                          initialdir = CONFIG['ROOT'],
                          filetypes = [('TIFF files', '.tif')])

opt_gsig=4
print(fnames[0])
test_min_corr_and_pnr(fnames[0], opt_gsig, start_frame=2000, end_frame=4000)


#Fiberscopic CNMF params from Cathie's MATLAB script
#NB!! Leave only those explicitly listed in the CNMF class    
gSiz = 17   # pixel, neuron diameter
gSig_fact = 0.28 
gSig = gSig_fact * gSiz # pixel, gaussian width of a gaussian kernel for filtering the data. 0 means no filtering
nb = 1      #             % number of background sources for each patch (only be used in SVD and NMF model)
ring_size_fact = 1.4 
merge_thr = 0.5
K = 300             # maximum number of neurons per patch. when K=[], take as many as possible.
min_corr = 0.7      # minimum local correlation for a seeding pixel
min_pnr = 36        # minimum peak-to-noise ratio for a seeding pixel



#MASK FOV!!!



'''
min_pixel_fact = 2.25;
min_pixel = min_pixel_fact* gSig^2    #minimum number of nonzero pixels for each neuron
'''


session_name = 'sleeppre'
#Classic miniscopic parameters
'''
gSig = 3
min_corr = 0.9
min_pnr = 8
gSiz = opt_gsig*4+1
nb = 0      
ring_size_fact = 1.5 
merge_thr = 0.8
K = None
'''
cnmf_dict= {'fr': 100,                   # frame rate, frames per second (NOW RECALCULATED FOR EACH FILE FROM TIMESTAMP DATA)
            'decay_time': 1,            # typical duration of calcium transient 
            'method_init': 'corr_pnr',  # use this for 1 photon
            'K': K,                     # upper bound on number of components per patch, in general None
            'gSig': (gSig, gSig),       # gaussian HALF-width of a 2D gaussian kernel (in pixels), which approximates a neuron
            'gSiz': (gSiz, gSiz),       # maximal radius of a neuron in pixels
            'merge_thr': merge_thr,     # merging threshold, max correlation allowed
            'p': 1,                     # order of the autoregressive system
            'tsub': 1,                  # downsampling factor in time for initialization
            'ssub': 1,                  # downsampling factor in space for initialization
            'rf': 40,                   # half-size of the patches in pixels. e.g., if rf=40, patches are 80x80
            'stride': 25,               # amount of overlap between the patches in pixels(keep it at least large as gSiz, i.e 4 times the neuron size gSig) 
            'only_init': True,          # set it to True to run CNMF-E
            'nb': nb,                    # number of background components (rank) if positive, else exact ring model with following settings: nb= 0: Return background as b and W, gnb=-1: Return full rank background B, gnb<-1: Don't return background
            'nb_patch': 0,              # number of background components (rank) per patch if nb>0, else it is set automatically
            'method_deconvolution': 'oasis',       # could use 'cvxpy' alternatively
            'low_rank_background': None,           # None leaves background of each patch intact, True performs global low-rank approximation if gnb>0
            'update_background_components': True,  # sometimes setting to False improve the results
            'min_corr': min_corr,                        # min peak value from correlation image
            'min_pnr': min_pnr,                         # min peak to noise ratio from PNR image
            'normalize_init': False,               # just leave as is
            'center_psf': True,                    # leave as is for 1 photon
            'ssub_B': 2,                           # additional downsampling factor in space for background
            'ring_size_factor': ring_size_fact,    # radius of ring is gSiz*ring_size_factor
            'del_duplicates': True,                # whether to remove duplicates from initialization
            'border_pix': 5,                       # number of pixels to not consider in the borders
            'min_SNR': 3,                          # adaptive way to set threshold on the transient size
            'rval_thr': 0.95,                      # threshold on space consistency           
            'use_cnn': False}                      # whether to use CNNs for event detection  

#session_config_path = get_session_config_path(session_name)
#cnmf_to_config = {'cnmf_params': cnmf_dict}
#update_config(cnmf_to_config, cpath=session_config_path)





import time
#fnames = glob(os.path.join(CONFIG['ROOT'], '*_MC.tif'))
#OR, alternatively, you can use manual file selection:
fnames = askopenfilenames(title = 'Select files for batch cnmf', initialdir = CONFIG['ROOT'], filetypes = [('TIFF files', '.tif')])

for name in tqdm.tqdm(fnames):
    fps = get_fps_from_timestamps(name[:-4-6], default_fps=100, verbose=False)
    #session_config_path = get_session_config_path(name[-12-30:-4-6-20])
    #cnmf_config = read_config(name=session_config_path)
    #cnmf_dict = cnmf_config['cnmf_params']
    cnmf_dict.update({'fr': fps})
    cnmf_dict.update({'tsub': 2})
    cnmf_dict.update({'ssub': 1})
    
    a = cnmf_dict['gSig'][0]
    aa = cnmf_dict['min_corr']
    aaa = cnmf_dict['min_pnr']
    out_name = name[:-4] + f'_gsig{a}_mincorr{aa}_minpnr{aaa}_estimates.pickle'
    print(out_name)
    DoCNMF(name,
           cnmf_dict,
           out_name=out_name,
           #start_frame=start_frame,
           #end_frame=end_frame,
           verbose=True)
    
    #CleanMemmaps(name)  

    err_cnt = 0
    while err_cnt < 100:
        try:
            CleanMemmaps(name)
            break
        except PermissionError:
            time.sleep(1)
            err_cnt += 1
    print('CleanMemmaps attempts:', err_cnt)





fname = "D:\\Projects\\estimates\\Trace\\Trace_H23_2D_gsig3_mincorr0.9_minpnr10_tsub2_estimates.pickle"



import copy
fname = askopenfilename(title = 'Select estimates file for examination',
                        initialdir = CONFIG['ROOT'],
                        filetypes = [('estimates files', '*estimates.pickle')])


def split_estimate(fname, default_fps=20, nparts=2):
    estimates0 = LoadEstimates(fname, default_fps=default_fps)
    chunks = np.array_split(estimates0.idx_components, nparts)
    for i, chunk in tqdm.tqdm(enumerate(chunks), total=len(chunks)):
        selected = chunk
        not_selected = np.array([comp for comp in estimates0.idx_components if comp not in chunk])
        
        estimates1 = copy.deepcopy(estimates0)
        estimates1.idx_components = selected.tolist()
        temp = estimates1.idx_components_bad.tolist() + not_selected.tolist()
        estimates1.idx_components_bad = np.sort(temp)
        
        base_name = fname.partition('_estimates')[0]
        out_name = base_name + f'_part_{i+1}_out_of_{nparts}_estimates.pickle'
        with open(out_name, "wb") as f:
            pickle.dump(estimates1, f)

split_estimate(fname, default_fps=20, nparts=2)



def merge_estimates(fnames, default_fps=20):
    all_estimates = []
    for fname in fnames:
        part_estimates = LoadEstimates(fname, default_fps=default_fps)
        all_estimates.append(part_estimates)

    all_good_comps = [est.idx_components]
    chunks = np.array_split(estimates0.idx_components, nparts)
    for i, chunk in tqdm.tqdm(enumerate(chunks), total=len(chunks)):
        selected = chunk
        not_selected = np.array([comp for comp in estimates0.idx_components if comp not in chunk])
        
        estimates1 = copy.deepcopy(estimates0)
        estimates1.idx_components = selected.tolist()
        temp = estimates1.idx_components_bad.tolist() + not_selected.tolist()
        estimates1.idx_components_bad = np.sort(temp)
        
        base_name = fname.partition('_estimates')[0]
        out_name = base_name + f'_part_{i+1}_out_of_{nparts}_estimates.pickle'
        with open(out_name, "wb") as f:
            pickle.dump(estimates1, f)


os.environ["BOKEH_ALLOW_WS_ORIGIN"] = 'localhost:8888'


import time
fname = askopenfilename(title = 'Select estimates file for examination',
                        initialdir = CONFIG['ROOT'],
                        filetypes = [('estimates files', '*estimates.pickle')])

bkapp_kwargs = {
    'start_frame': 0,          # start from this frame
    'end_frame': 90000,          # end at this frame
    'downsampling': 1,           # take every 'x' frame
    'fill_alpha': 0.5,       # selected neuron transparency
    'ns_alpha': 0.15,         # non-selected neuron transparency
    'line_width': 0.5,         # border width
    'cthr': 0.5,             # coutour_thr from caiman (% of signal inside a patch), affects patch size
    'line_alpha': 0.5,           # border transparency
    'trace_line_width': 0.5,       # trace line width
    'trace_alpha': 1,          # trace transparency
    'size': 520,
    'button_width': 120,         # button width in pixels
    'verbose': 0,
    'enable_gpu_backend': 1,
    'oh_shit': 0
}
ExamineCells(fname, default_fps=30, bkapp_kwargs=bkapp_kwargs)


fnames = askopenfilenames(title = 'Select files for batch cnmf',
                          initialdir = CONFIG['ROOT'],
                          filetypes = [('TIFF files', '.tif')])

#fnames = glob(os.path.join(CONFIG['ROOT'], '*_MC.tif'))

ManualSeeds(fnames[0], size=800, cnmf_dict=None)





s_names = glob(os.path.join(CONFIG['ROOT'], '*seeds.pickle'))
#OR, alternatively, you can use manual file selection:
#s_names = askopenfilenames(title = 'Select seeds files for re-CNMFing', initialdir = CONFIG['ROOT'], filetypes = [('seeds files', '*seeds.pickle')])


for s_name in s_names:
    base_name = s_name.partition('_seeds')[0][:-4]
    
    e_names = glob(base_name + '_estimates.pickle')
    tif_names = glob(base_name + '.tif')
    ReDoCNMF(s_name, e_name=None, tif_name=tif_names[0], cnmf_dict=cnmf_dict)
    CleanMemmaps(base_name)





#fnames = glob(CONFIG['ROOT'] + '*traces.csv')
#OR, alternatively, you can use manual file selection:
fnames = askopenfilenames(title = 'Select traces for event detection', initialdir = CONFIG['ROOT'], filetypes = [('traces files', '*traces.csv')])

sd_dict = {'thr': 4,        #threshold for peaks in Median Absolute Deviations (MADs)                   
           'sigma' : 5,     #smoothing parameter for peak detection, frames
           'est_ton' : 0.05, #estimated event rising time, s
           'est_toff' : 0.1,  #estimated event decay time, s
           'draw_details': True} #whether to draw smoothed traces, peaks, pits and fits 

for name in fnames:
    FitEvents(name, opts = sd_dict)






#fnames = glob(CONFIG['ROOT'] + '*traces.csv')
fnames = askopenfilenames(title = 'Select traces and events for depicting', initialdir = CONFIG['ROOT'], filetypes = [('traces files', '*traces.csv')])

for name in fnames:
    DrawSpEvents(name, name.replace('traces','spikes'))






def read_traces(fname):
    trdata = pd.read_csv(fname)
    time = trdata['time_s'].values
    traces = np.array(trdata)[:,1:].T
    return traces, time

def read_traces_np(fname):  #This is in case the first row is NOT a header, i.e marked with 'Time_s' etc
    trdata = np.genfromtxt(fname, delimiter = ',')
    time = trdata[:,0]
    traces = trdata[:,1:].T
    return traces, time

#fnames = glob(CONFIG['ROOT'] + '*traces.csv')
fnames = askopenfilenames(title = 'Select traces for event detection', initialdir = CONFIG['ROOT'], filetypes = [('traces files', '*traces.csv')])

wvt_param_dict = {'fps': 100,        # fps, frames                   
                  'sigma' : 3,      # smoothing parameter for peak detection, frames
                  'beta' : 2,       # Generalized Morse Wavelet parameter, FIXED
                  'gamma' : 3,      # Generalized Morse Wavelet parameter, FIXED
                  'eps': 3,         # spacing between consecutive events, frames
                  'manual_scales': np.logspace(2.5,5.5,50, base=2),

                  # ridge filtering params
                  'scale_length_thr': 10,  # min number of scales where ridge is present thr, higher = less events. max=len(manual_scales)
                  'max_scale_thr': 7,      # index of a scale with max ridge intensity thr, higher = less events. < 5 = noise, > 20 = huge events
                  'max_ampl_thr': 0.05,    # max ridge intensity thr, higher = less events. < 5 = noise, > 20 = huge events
                  'max_dur_thr': 200,      # max event duration thr, higher = more events (but probably strange ones)
}


for fname in fnames:
    traces, time = read_traces_np(fname)
    st_evinds, end_evinds, all_ridges = extract_wvt_events(traces, wvt_param_dict)
    events_to_csv(time, st_evinds, end_evinds, fname)





wvt_param_dict['scale_length_thr'] = 40,  # min number of scales where ridge is present thr, higher = less events. max=len(manual_scales)
wvt_param_dict['max_scale_thr'] = 7,      # index of a scale with max ridge intensity thr, higher = less events. < 5 = noise, > 20 = huge events
wvt_param_dict['max_ampl_thr'] = 0.05,    # max ridge intensity thr, higher = less events. < 5 = noise, > 20 = huge events
wvt_param_dict['max_dur_thr'] = 200,      # max event duration thr, higher = more events (but probably strange ones)

events = []
for i in range(traces.shape[0]):
    st_evinds, end_evinds = get_events_from_ridges(all_ridges[i],
                                                   scale_length_thr=40,
                                                   max_scale_thr=10,
                                                   max_ampl_thr=0.05,
                                                   max_dur_thr=100)

    events.append(end_evinds)


st = 0
end = 10000
neuron_ind = 40

sig = gaussian_filter1d(traces[neuron_ind], sigma=wvt_param_dict['sigma'])

fig, ax = plt.subplots(figsize=(10,8))
ax.set_xlim(st, end)
ax.plot(np.arange(st, end), sig[st:end], c='b')

for ev in end_evinds[neuron_ind]:
    ax.axvline(ev, c='r', alpha=0.5)


events_to_csv(time, st_evinds, end_evinds, fname)


fname
